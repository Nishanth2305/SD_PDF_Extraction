{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd22a43f-616c-4272-9249-cec64b3d5ac6",
   "metadata": {},
   "source": [
    "# Import packages and Folder path"
   ]
  },
  {
   "cell_type": "code",
   "id": "fef25dff-4793-496b-ae0e-8f9822337d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.301880Z",
     "start_time": "2025-03-28T09:57:17.281546Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import camelot\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 143
  },
  {
   "cell_type": "markdown",
   "id": "7fcdce30-9ae7-44c7-9e68-69967a305e02",
   "metadata": {},
   "source": [
    "# Extracting BSN and Version details"
   ]
  },
  {
   "cell_type": "code",
   "id": "e8b73f31-6600-41bb-9b2a-7aaab5f30578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.368595Z",
     "start_time": "2025-03-28T09:57:17.344662Z"
    }
   },
   "source": [
    "# Function to extract table from a single PDF\n",
    "def extract_bsn_table_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # print(f\"\\nExtracting BSN table from '{os.path.basename(pdf_path)}'...\")\n",
    "        tables = camelot.read_pdf(pdf_path, pages=\"1\", flavor=\"lattice\")\n",
    "        if tables:\n",
    "            return tables[0].df  # Return the first table as a DataFrame\n",
    "        else:\n",
    "            print(f\"\\nNo tables found in '{pdf_path}'.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing '{pdf_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to extract \"Service Description ID\" from a table\n",
    "def extract_bsn_number_from_table(table_df):\n",
    "    if table_df is not None:\n",
    "        table_df.columns = [\"Name\", \"Value\"]  # Rename columns\n",
    "        table_df = table_df.map(lambda x: str(x).replace(\" \", \"\").strip() if pd.notna(x) else x)  # Remove extra spaces\n",
    "        if \"ServiceDescriptionID:\" in table_df[\"Name\"].values:\n",
    "            bsn_value = table_df.loc[table_df[\"Name\"] == \"ServiceDescriptionID:\", \"Value\"].values[0]\n",
    "\n",
    "            # Check if the value already starts with 'BSN'\n",
    "            if not bsn_value.startswith(\"BSN\"):\n",
    "                bsn_value = \"BSN\" + bsn_value\n",
    "\n",
    "            print(f\"Extracted BSN Number: {bsn_value}\")\n",
    "            return bsn_value\n",
    "        else:\n",
    "            print(\"Service Description ID not found in table.\\n\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to extract \"Service Description ID\" from a table\n",
    "def extract_version_number_from_table(table_df):\n",
    "    version_match = \"v01.40\"\n",
    "    value = False\n",
    "    if table_df is not None:\n",
    "        table_df.columns = [\"Name\", \"Value\"]  # Rename columns\n",
    "        table_df = table_df.map(str.strip)  # Remove leading/trailing spaces\n",
    "\n",
    "        if \"SD Template Version:\" in table_df[\"Name\"].values:\n",
    "            version_number = table_df.loc[table_df[\"Name\"] == \"SD Template Version:\", \"Value\"].values[0]\n",
    "\n",
    "            # Check if the SD file is in new version\n",
    "            if version_number == version_match:\n",
    "                value = True\n",
    "        else:\n",
    "            print(\"\\nSD Version is not found\")\n",
    "\n",
    "    return value"
   ],
   "outputs": [],
   "execution_count": 144
  },
  {
   "cell_type": "markdown",
   "id": "db0816f5-5447-4c6c-b4b6-3f653a4693b9",
   "metadata": {},
   "source": [
    "# Function to match regex search text"
   ]
  },
  {
   "cell_type": "code",
   "id": "f337ff77-df4b-4e98-849b-a5409c3afd61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.412317Z",
     "start_time": "2025-03-28T09:57:17.399388Z"
    }
   },
   "source": [
    "# Function to normalize text for accurate searching\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text.strip())"
   ],
   "outputs": [],
   "execution_count": 145
  },
  {
   "cell_type": "markdown",
   "id": "41de944c-fe5d-44a5-8689-0de402a7bf41",
   "metadata": {},
   "source": [
    "# Extracting the Index page number"
   ]
  },
  {
   "cell_type": "code",
   "id": "57f2a6ff-4828-41d5-9d74-faf3d3b273a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.452744Z",
     "start_time": "2025-03-28T09:57:17.440008Z"
    }
   },
   "source": [
    "# Function to get the index page number\n",
    "def find_index_page_number(pdf_path):\n",
    "    # Open the PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "\n",
    "    # Initialize variables\n",
    "    index_page_number = None\n",
    "\n",
    "    # Regex patterns\n",
    "    list_of_tables_pattern = re.compile(r'List of Tables|List of Tables and Figures', re.IGNORECASE)\n",
    "\n",
    "    # Detect and skip the 'List of Tables' page\n",
    "    for page_num in range(total_pages):\n",
    "        page_text = reader.pages[page_num].extract_text()\n",
    "        if list_of_tables_pattern.search(page_text):\n",
    "            index_page_number = page_num + 1  # Start search after this page\n",
    "            break\n",
    "\n",
    "    if index_page_number is not None:\n",
    "        return index_page_number"
   ],
   "outputs": [],
   "execution_count": 146
  },
  {
   "cell_type": "markdown",
   "id": "de1a47d1-16c8-4d50-90cb-91f8d9877bde",
   "metadata": {},
   "source": [
    "# Extract list of pages from search text "
   ]
  },
  {
   "cell_type": "code",
   "id": "3c49ff04-3d36-4359-a069-7a80cbceecde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.486596Z",
     "start_time": "2025-03-28T09:57:17.478110Z"
    }
   },
   "source": [
    "# Function to find all the occurrences of service availability text\n",
    "def find_all_service_availability_and_support_hour_pages(pdf_path, search_text):\n",
    "    occurrences = []  # List to store all occurrences\n",
    "    compiled_pattern = re.compile(search_text, re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):  # Pages are 1-indexed\n",
    "                text = page.extract_text()\n",
    "                if text:  # Ensure the page contains text\n",
    "                    normalized_text = normalize_text(text)  # Normalize text\n",
    "\n",
    "                    if compiled_pattern.search(normalized_text):\n",
    "                        occurrences.append(page_number)  # Store the page number\n",
    "\n",
    "        # If no occurrences found, return an empty list\n",
    "        if not occurrences:\n",
    "            print(f\"\\nNo occurrences of '{search_text}' found in '{os.path.basename(pdf_path)}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing '{os.path.basename(pdf_path)}': {e}\")\n",
    "\n",
    "    return occurrences  # Return the full list of occurrences"
   ],
   "outputs": [],
   "execution_count": 147
  },
  {
   "cell_type": "markdown",
   "id": "12d9e0f7-0d91-42fd-b1bd-bedb13ea3501",
   "metadata": {},
   "source": [
    "# Generate list of pages"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b62f45e-126b-47db-9967-a09e9cac8321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.505710Z",
     "start_time": "2025-03-28T09:57:17.495864Z"
    }
   },
   "source": [
    "# Function to generate a continuous page list\n",
    "def create_page_list(pdf_path, search_start_text, search_end_text):\n",
    "    index_page_number = find_index_page_number(pdf_path)\n",
    "\n",
    "    start_pages = find_all_service_availability_and_support_hour_pages(pdf_path, search_start_text)\n",
    "    end_pages = find_all_service_availability_and_support_hour_pages(pdf_path, search_end_text)\n",
    "\n",
    "    # Remove index_page_number if present in both lists\n",
    "    if index_page_number in start_pages and index_page_number in end_pages:\n",
    "        start_pages.remove(index_page_number)\n",
    "        end_pages.remove(index_page_number)\n",
    "\n",
    "    # Remove any pages that are less than index_page_number\n",
    "    start_pages = [page for page in start_pages if page >= index_page_number]\n",
    "    end_pages = [page for page in end_pages if page >= index_page_number]\n",
    "\n",
    "    # Ensure material_end_pages do not contain pages lower than the lowest start page\n",
    "    if start_pages:\n",
    "        lowest_start_page = min(start_pages)\n",
    "        end_pages = [page for page in end_pages if page >= lowest_start_page]\n",
    "\n",
    "    # Get the continuous range of page numbers\n",
    "    if start_pages and end_pages:\n",
    "        lowest_page = min(start_pages + end_pages)\n",
    "        highest_page = max(start_pages + end_pages)\n",
    "        page_numbers = list(range(lowest_page, highest_page + 1))\n",
    "    else:\n",
    "        page_numbers = []  # No valid range if lists are empty\n",
    "\n",
    "    return start_pages, end_pages, page_numbers"
   ],
   "outputs": [],
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "id": "f4186b0b-152f-4da3-b47d-0968658d74e1",
   "metadata": {},
   "source": [
    "# Extracting SA Material and Availability details"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e2f3a86-6020-4528-922c-addd8a42d0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.527206Z",
     "start_time": "2025-03-28T09:57:17.513338Z"
    }
   },
   "source": [
    "# Function to extract data from the material tables\n",
    "def extract_data_from_material_tables(pdf_path, page_numbers):\n",
    "    extracted_data = {}  # Dictionary to store key-value pairs\n",
    "    extracted_dataframes = []\n",
    "\n",
    "    try:\n",
    "        for page_number in page_numbers:\n",
    "            page_number_str = str(page_number)  # Camelot requires page numbers as a string\n",
    "\n",
    "            # Extract tables using Camelot (lattice method for structured tables)\n",
    "            tables = camelot.read_pdf(pdf_path, pages=page_number_str, flavor='lattice', line_scale=50)\n",
    "\n",
    "            if not tables or tables.n == 0:\n",
    "                return extracted_data\n",
    "\n",
    "            else:\n",
    "                for i in range(tables.n):\n",
    "                    df = tables[i].df  # Convert to DataFrame\n",
    "                    df = df.replace('\\n', ' ', regex=True)  # Clean newlines\n",
    "                    df = df.applymap(\n",
    "                        lambda x: x.strip().replace(\"“\", \"\").replace(\"”\", \"\").replace('\"', ''))  # Normalize Text\n",
    "\n",
    "                    # Normalize column headers by removing hidden quotes & spaces\n",
    "                    cleaned_headers = [col.strip().replace(\"“\", \"\").replace(\"”\", \"\").replace('\"', '') for col in\n",
    "                                       df.iloc[0].values]\n",
    "                    cleaned_headers = [\n",
    "                        re.search(r'\\bService Availability\\b', col).group(0) if re.search(r'\\bService Availability\\b',\n",
    "                                                                                          col) else col for col in\n",
    "                        cleaned_headers]\n",
    "                    # print(cleaned_headers)\n",
    "\n",
    "                    if \"Service Availability\" in cleaned_headers:\n",
    "                        # Make second row as the header\n",
    "                        df.columns = cleaned_headers  # Assign new headers\n",
    "                        df = df[1:].reset_index(drop=True)  # Drop the first row\n",
    "                        extracted_dataframes.append(df)\n",
    "\n",
    "                        num_columns = df.shape[1]  # Number of columns in the dataframe\n",
    "\n",
    "                        # Logic for 2 columns table\n",
    "                        if num_columns == 2:\n",
    "                            val1 = df.iloc[0, 0]\n",
    "                            val2 = df.iloc[1, 0]\n",
    "\n",
    "                            if re.search(r'\\b\\d{6}\\b', val2):\n",
    "                                key = val1 + ' ' + val2\n",
    "                            else:\n",
    "                                key = val1\n",
    "                            # key = df.iloc[0, 0]\n",
    "                            value = df.iloc[1, 1]\n",
    "\n",
    "                            cleaned_value = re.sub(r'\\bPI\\s*(?=\\d|[^\\w\\s])', 'KPI', value,\n",
    "                                                   flags=re.IGNORECASE).strip()  # Convert PI to KPI first\n",
    "                            cleaned_value = re.sub(r'\\b[A-Za-z]\\b', '',\n",
    "                                                   cleaned_value).strip()  # Remove single characters\n",
    "                            cleaned_value = re.sub(r'(?<=\\d) (?=\\d)', '',\n",
    "                                                   cleaned_value).strip()  # Remove spaces between numbers\n",
    "\n",
    "                            # If value is empty, check the next available row dynamically\n",
    "                            if not cleaned_value:\n",
    "                                for j in range(1, len(df)):  # Iterate through remaining rows\n",
    "                                    temp_value = df.iloc[j, 1].strip()\n",
    "\n",
    "                                    if temp_value:  # If a valid value is found, use it\n",
    "                                        cleaned_value = temp_value\n",
    "                                        break\n",
    "\n",
    "                            cleaned_value = re.sub(r' {2,}', ' ', cleaned_value)\n",
    "                            key = key.replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"   \", \" \").strip()\n",
    "\n",
    "                            if key in extracted_data:\n",
    "                                # Convert existing value to a list if it's a string\n",
    "                                if isinstance(extracted_data[key], str):\n",
    "                                    extracted_data[key] = [extracted_data[key]]  # Convert string to list\n",
    "\n",
    "                                extracted_data[key].append(cleaned_value)  # Append new value to the list\n",
    "                            else:\n",
    "                                extracted_data[key] = cleaned_value  # Store first value as a string\n",
    "\n",
    "\n",
    "                        # Logic for single-column tables\n",
    "                        elif num_columns == 1:\n",
    "                            key = df.iloc[0, 0]\n",
    "\n",
    "                            # Find the row containing \"Service Level Target Value\" or similar keywords\n",
    "                            value = \"\"\n",
    "                            for row in df.iloc[:, 0]:  # Iterate over the single column\n",
    "                                match = re.search(\n",
    "                                    r\"(Service Level Target Value|SL Target Value|Target Value)\\s*[:,]?\\s*(=\\s*\\d+[.,]?\\d*\\s*%)\",\n",
    "                                    row, re.IGNORECASE)\n",
    "                                if match:\n",
    "                                    value = match.group(2).strip()  # Extract the percentage value\n",
    "                                    break  # Stop after finding the first match\n",
    "\n",
    "                            key = key.replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"   \", \" \").strip()\n",
    "\n",
    "                            if key in extracted_data:\n",
    "                                # Convert existing value to a list if it's a string\n",
    "                                if isinstance(extracted_data[key], str):\n",
    "                                    extracted_data[key] = [extracted_data[key]]  # Convert string to list\n",
    "\n",
    "                                extracted_data[key].append(value)  # Append new value to the list\n",
    "                            else:\n",
    "                                extracted_data[key] = value  # Store first value as a string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing the PDF file: {e}\")\n",
    "\n",
    "    return extracted_data"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "cell_type": "markdown",
   "id": "8a58c8aa-ed77-4f83-a60f-1453a5720ce2",
   "metadata": {},
   "source": [
    "# Extracting Incident Response & Resolution time details"
   ]
  },
  {
   "cell_type": "code",
   "id": "69238b05-ab09-4790-b5c7-6e4e9ffbf6f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.555649Z",
     "start_time": "2025-03-28T09:57:17.534479Z"
    }
   },
   "source": [
    "# Function to find the first or second occurrence of the text\n",
    "def find_incident_table_page_number(pdf_path, search_text):\n",
    "    occurrences = []  # Track pages where the search text is found\n",
    "    compiled_pattern = re.compile(search_text, re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):  # Pages are 1-indexed\n",
    "                text = page.extract_text()\n",
    "                if text:  # Ensure the page contains text\n",
    "                    normalized_text = normalize_text(text)  # Normalize text\n",
    "\n",
    "                    match = compiled_pattern.search(normalized_text)\n",
    "                    if match:\n",
    "                        occurrences.append(page_number)\n",
    "\n",
    "                        # Stop when the second occurrence is found\n",
    "                        if len(occurrences) == 2:\n",
    "                            return page_number\n",
    "\n",
    "            # Handle the case where there is only one occurrence\n",
    "            if len(occurrences) == 1:\n",
    "                return occurrences[0]\n",
    "\n",
    "        # print(f\"\\nNo occurrences of DRC search text found in '{os.path.basename(pdf_path)}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing '{os.path.basename(pdf_path)}': {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to convert dataframe into list\n",
    "def convert_df_into_list(extracted_dataframe):\n",
    "\n",
    "    if extracted_dataframe is not None:\n",
    "        # **Detect Table Format**\n",
    "        # noinspection PyUnusedLocal\n",
    "        headers = [col.replace(\"\\n\", \"\").lower().strip() for col in extracted_dataframe.columns]\n",
    "\n",
    "        # **Remove P1, P2, P3, P4 Row (Second Row in Most Cases)**\n",
    "        if extracted_dataframe.shape[0] > 1:\n",
    "            first_col_values = extracted_dataframe.iloc[:, 0].astype(str).str.lower().str.strip().str.replace(\"\\n\", \"\",\n",
    "                                                                                                              regex=True)\n",
    "            if first_col_values.iloc[0] == \"nan\":  # Classification row detected\n",
    "                extracted_dataframe = extracted_dataframe.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "        # **Initialize Empty Lists**\n",
    "        response_time_list, resolution_time_list = [], []\n",
    "\n",
    "        # **Identify Available Rows in the First Column**\n",
    "        first_col_values = extracted_dataframe.iloc[:, 0].astype(str).str.lower().str.strip().str.replace(\"\\n\", \"\",\n",
    "                                                                                                          regex=True)\n",
    "\n",
    "        # If \"Incident Response Time\" is present, extract values\n",
    "        if \"incident response time\" in first_col_values.values:\n",
    "            response_index = first_col_values[first_col_values == \"incident response time\"].index[0]\n",
    "            response_time_list = [\n",
    "                re.sub(r' {2,}', ' ', val.replace(\"\\n\", \"\").replace(\"•\", \"\").replace(\"\\uf0b7\", \"\").strip())\n",
    "                for val in extracted_dataframe.iloc[response_index, 1:]]\n",
    "\n",
    "        # If \"Incident Resolution Time\" is present, extract values\n",
    "        if \"incident resolution time\" in first_col_values.values:\n",
    "            resolution_index = first_col_values[first_col_values == \"incident resolution time\"].index[0]\n",
    "            resolution_time_list = [\n",
    "                re.sub(r' {2,}', ' ', val.replace(\"\\n\", \"\").replace(\"•\", \"\").replace(\"\\uf0b7\", \"\").strip())\n",
    "                for val in extracted_dataframe.iloc[resolution_index, 1:]]\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nNo Incident values were extracted from the PDF.\")\n",
    "        response_time_list, resolution_time_list = [], []\n",
    "\n",
    "    return response_time_list, resolution_time_list\n",
    "\n",
    "\n",
    "# Function to extract all the tables from the target page\n",
    "def extract_all_tables_from_incident_page(pdf_path, page_number):\n",
    "    try:\n",
    "\n",
    "        expected_headers = [\"classification\", \"incident response time\", \"incident resolution time\",\n",
    "                            \"incident classification\", \"response time\", \"resolution time\"]\n",
    "        row_keywords = [\"incident resolution time\", \"incident response time\", \"response time\", \"resolution time\"]\n",
    "\n",
    "        # Camelot requires page numbers as a string\n",
    "        page_number_str = str(page_number)\n",
    "\n",
    "        # Extract tables using Camelot (lattice method for structured tables)\n",
    "        tables = camelot.read_pdf(pdf_path, pages=page_number_str, flavor='lattice', line_scale=50)\n",
    "\n",
    "        if not tables or tables.n == 0:\n",
    "            print(\"\\nNo tables found on the page.\")\n",
    "            return None\n",
    "\n",
    "        # Convert each table to a DataFrame\n",
    "        dataframes = []\n",
    "        for i, table in enumerate(tables):\n",
    "            try:\n",
    "                # Convert table to DataFrame\n",
    "                df = table.df  # Camelot returns tables as pandas DataFrames\n",
    "                df.columns = [col.strip() for col in df.iloc[0]]  # Set headers\n",
    "                df = df[1:]  # Remove header row from data\n",
    "                df = df.reset_index(drop=True)  # Reset index\n",
    "\n",
    "                # Drop empty columns\n",
    "                df = df.dropna(how=\"all\", axis=1)\n",
    "\n",
    "                if not df.empty:\n",
    "                    dataframes.append(df)\n",
    "                else:\n",
    "                    print(f\"\\nTable {i + 1} is empty after cleaning. Skipping...\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing Table {i + 1}: {e}\")\n",
    "\n",
    "        # If no valid table found, return None\n",
    "        if not dataframes:\n",
    "            return None\n",
    "\n",
    "            # # Logic to return the required DataFrame\n",
    "        # if len(dataframes) <= 3:\n",
    "        #     return dataframes[0]\n",
    "\n",
    "        # elif len(dataframes) == 3:\n",
    "        #     return dataframes[2]\n",
    "\n",
    "        # else:\n",
    "        #     return None\n",
    "\n",
    "        for df in dataframes:\n",
    "            # Normalize headers: Convert to lowercase and strip spaces\n",
    "            headers = [str(col).replace(\"\\n\", \"\").strip().lower() for col in df.columns]\n",
    "            cleaned_row_keywords = [str(val).replace(\"\\n\", \"\").strip().lower() for val in row_keywords]\n",
    "\n",
    "            # Step 1: Check if any expected header is in table headers\n",
    "            if any(keyword in headers for keyword in expected_headers):\n",
    "                return df  # Return the first valid table\n",
    "\n",
    "            # Step 2: If no matching header, check the rows for keywords\n",
    "            for _, row in df.iterrows():\n",
    "                row_text = \" \".join(map(str, row.values)).replace(\"\\n\", \"\").strip().lower()\n",
    "                # print(row_text)  # Debugging to check the cleaned row text\n",
    "\n",
    "                if any(keyword in row_text for keyword in cleaned_row_keywords):\n",
    "                    return df  # Return table if a row contains the keywords\n",
    "\n",
    "            return None  # Return None if no matching table is found\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing the PDF file: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "id": "49f1b948-db9a-4099-b8b7-86a770e05834",
   "metadata": {},
   "source": [
    "# Extracting DRC details"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c585b81-922a-4ad7-b5f5-6e70ba029c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.594353Z",
     "start_time": "2025-03-28T09:57:17.570478Z"
    }
   },
   "source": [
    "# Function to find the first or second occurrence of the text\n",
    "def find_drc_table_page_number(pdf_path, search_text):\n",
    "    occurrences = []  # Track pages where the search text is found\n",
    "    compiled_pattern = re.compile(search_text, re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):  # Pages are 1-indexed\n",
    "                text = page.extract_text()\n",
    "                if text:  # Ensure the page contains text\n",
    "                    normalized_text = normalize_text(text)  # Normalize text\n",
    "\n",
    "                    match = compiled_pattern.search(normalized_text)\n",
    "                    if match:\n",
    "                        occurrences.append(page_number)\n",
    "\n",
    "                        # Stop when the second occurrence is found\n",
    "                        if len(occurrences) == 2:\n",
    "                            return page_number\n",
    "\n",
    "            # Handle the case where there is only one occurrence\n",
    "            if len(occurrences) == 1:\n",
    "                return occurrences[0]\n",
    "\n",
    "        # print(f\"\\nNo occurrences of DRC search text found in '{os.path.basename(pdf_path)}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing '{os.path.basename(pdf_path)}': {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to convert dataframe into dictionary\n",
    "def convert_df_to_dict(dataframes):\n",
    "    extracted_data = {}  # Final dictionary to store results\n",
    "\n",
    "    # Define the expected column names for different formats\n",
    "    required_formats = [\n",
    "        [\"Applicable DRC\", \"Applicable RPO\"],\n",
    "        [\"Applicable DRC\", \"Applicable RTO/RPO\"],\n",
    "        [\"Applicable DRCI\", \"Applicable RPO\"],\n",
    "        [\"Applicable DRCI\", \"Applicable RTO/RPO\"]\n",
    "    ]\n",
    "\n",
    "    for df in dataframes:\n",
    "        # Ensure DataFrame is not empty\n",
    "        if df is None or df.empty:\n",
    "            continue  # Skip empty DataFrames\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = [re.sub(r'\\s{2,}', ' ', col.replace(\"\\n\", \" \")).strip() for col in df.columns]\n",
    "\n",
    "        # Processing Format 1\n",
    "        if len(df.columns) == 3 and list(df.columns[1:]) in required_formats:\n",
    "            for _, row in df.iterrows():\n",
    "                key = row.iloc[0].replace(\"\\n\", \"\").strip()\n",
    "\n",
    "                drc_raw = str(row.iloc[1]) if pd.notna(row.iloc[1]) else \"\"\n",
    "                rpo_raw = str(row.iloc[2]) if pd.notna(row.iloc[2]) else \"\"\n",
    "\n",
    "                # drc_values = [chunk.strip() for chunk in drc_raw.split(\"\\n\") if chunk.strip()]\n",
    "                # rpo_values = [chunk.strip() for chunk in rpo_raw.split(\"\\n\") if chunk.strip()]\n",
    "\n",
    "                drc_values = [\"\".join(drc_raw.splitlines()).strip()]\n",
    "                rpo_values = [\"\".join(rpo_raw.splitlines()).strip()]\n",
    "\n",
    "                # Initialize dictionary entry if key is new\n",
    "                if key not in extracted_data:\n",
    "                    extracted_data[key] = {\"Applicable DRC\": [], \"Applicable RPO\": []}\n",
    "\n",
    "                extracted_data[key][\"Applicable DRC\"].extend(drc_values)\n",
    "                extracted_data[key][\"Applicable RPO\"].extend(rpo_values)\n",
    "\n",
    "        # Processing Format 2\n",
    "        elif (df.columns[0] == \"\" or\n",
    "              all(df.iloc[:, 1:].applymap(\n",
    "                  lambda x: bool(re.search(r'\\bYES\\b|\\bNO\\b|\\bNA\\b|\\bN/A\\b', str(x), re.IGNORECASE))\n",
    "              ).all(axis=1))):\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                key = row.iloc[0].replace(\"\\n\", \"\").strip()\n",
    "                key = re.sub(r' {2,}', ' ', key)\n",
    "\n",
    "                # Initialize dictionary entry if key is new\n",
    "                if key not in extracted_data:\n",
    "                    extracted_data[key] = {\"Applicable DRC\": [], \"Applicable RPO\": []}\n",
    "\n",
    "                # Iterate over columns to extract values\n",
    "                for col, val in zip(df.columns[1:], row.iloc[1:]):\n",
    "                    normalized_col = col.replace(\"\\n\", \" \").strip()\n",
    "                    if \"yes\" in str(val).strip().lower():\n",
    "                        if \"DRC\" in normalized_col or \"EDR\" in normalized_col:\n",
    "                            extracted_data[key][\"Applicable DRC\"].append(normalized_col)\n",
    "                        elif \"RPO\" in normalized_col:\n",
    "                            extracted_data[key][\"Applicable RPO\"].append(normalized_col)\n",
    "\n",
    "    return extracted_data  # Returns a valid dictionary ({} if no matches found)\n",
    "\n",
    "\n",
    "# Function to extract all the tables from the target page\n",
    "def extract_all_tables_from_drc_page(pdf_path, page_number):\n",
    "    try:\n",
    "        # Define the expected headers for the relevant table\n",
    "        possible_last_columns = [\"Applicable RPO\", \"Applicable RTO/RPO\"]\n",
    "        required_first_column = \"Applicable DRC\"\n",
    "\n",
    "        # Camelot requires page numbers as a string\n",
    "        page_number_str = str(page_number)\n",
    "\n",
    "        # Extract tables using Camelot (lattice method for structured tables)\n",
    "        tables = camelot.read_pdf(pdf_path, pages=page_number_str, flavor='lattice', line_scale=50)\n",
    "\n",
    "        if not tables or tables.n == 0:\n",
    "            print(\"\\nNo tables found on the page.\")\n",
    "            return None\n",
    "\n",
    "        # Convert each table to a DataFrame\n",
    "        dataframes = []\n",
    "        for i, table in enumerate(tables):\n",
    "            try:\n",
    "                # Convert table to DataFrame\n",
    "                df = table.df  # Camelot returns tables as pandas DataFrames\n",
    "                df.columns = [col.strip() for col in df.iloc[0]]  # Set headers\n",
    "                df = df[1:]  # Remove header row from data\n",
    "                df = df.reset_index(drop=True)  # Reset index\n",
    "                df = df.dropna(how=\"all\", axis=1)  # Drop empty columns\n",
    "\n",
    "                if not df.empty:\n",
    "                    dataframes.append(df)\n",
    "                else:\n",
    "                    print(f\"\\nTable {i + 1} is empty after cleaning. Skipping...\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing Table {i + 1}: {e}\")\n",
    "\n",
    "        # If no valid table found, return None\n",
    "        if not dataframes:\n",
    "            return None\n",
    "\n",
    "        # Check for split table by inspecting the previous page\n",
    "        combined_dataframes = []\n",
    "        for target_df in dataframes:\n",
    "            # Ensure the last two columns match the required headers\n",
    "            if target_df.shape[1] >= 2 and target_df.columns[-2] == required_first_column and target_df.columns[\n",
    "                -1] in possible_last_columns:\n",
    "                # Check the previous page\n",
    "                previous_page = page_number - 1\n",
    "                previous_page_str = str(previous_page)\n",
    "                previous_tables = camelot.read_pdf(pdf_path, pages=previous_page_str, flavor='lattice', line_scale=50)\n",
    "\n",
    "                if previous_tables.n > 0:  # Ensure tables exist\n",
    "                    for prev_table in previous_tables:\n",
    "                        try:\n",
    "                            df_prev = prev_table.df\n",
    "                            if df_prev.shape[0] > 1:  # Ensure at least one row\n",
    "                                df_prev.columns = [col.strip() for col in df_prev.iloc[0]]  # Set headers\n",
    "                                df_prev = df_prev[1:].reset_index(drop=True)  # Remove header row\n",
    "                                df_prev = df_prev.dropna(how=\"all\", axis=1)  # Drop empty columns\n",
    "\n",
    "                                # Match headers with the target DataFrame\n",
    "                                if df_prev.shape[1] >= 2 and df_prev.columns[-2] == required_first_column and \\\n",
    "                                        df_prev.columns[-1] in possible_last_columns:\n",
    "                                    # Combine previous page's table with target page's table\n",
    "                                    combined_table = pd.concat([df_prev, target_df], ignore_index=True)\n",
    "                                    combined_dataframes.append(combined_table)\n",
    "                                    continue  # Move to the next table\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\nError processing table from the previous page: {e}\")\n",
    "\n",
    "                # If no split table was found, store the target_df separately\n",
    "                combined_dataframes.append(target_df)\n",
    "\n",
    "        # If combined_dataframes has valid data, return it; otherwise, return all extracted tables\n",
    "        return combined_dataframes if combined_dataframes else dataframes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing the PDF file: {e}\")\n",
    "        return []"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "cell_type": "markdown",
   "id": "f26a0af0-bc3b-4cd2-9a1c-3c172f921d6b",
   "metadata": {},
   "source": [
    "# Extracting Support Hour details from Service Timing table"
   ]
  },
  {
   "cell_type": "code",
   "id": "8657779c-1085-437c-abcb-76884e1ce399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.662493Z",
     "start_time": "2025-03-28T09:57:17.633593Z"
    }
   },
   "source": [
    "# Function to extract the support hour value\n",
    "def extract_support_hours(df):\n",
    "    # Dictionary to store extracted values\n",
    "    support_dict = {\n",
    "        \"1st Level Support\": \"\",\n",
    "        \"2nd Level Support\": \"\",\n",
    "        \"Emergency Support\": \"\",\n",
    "        \"Non-Emergency Support\": \"\"\n",
    "    }\n",
    "\n",
    "    if df.empty or df.shape[1] < 2:  # Ensure valid DataFrame with at least 2 columns\n",
    "        return support_dict  # Return empty structure if input is invalid\n",
    "\n",
    "    # Normalize the first column (remove spaces, convert to lowercase, but keep special characters)\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].astype(str).fillna(\"\").str.replace(\"\\n\", \"\").str.lower()\n",
    "\n",
    "    # Define keyword mappings to corresponding dictionary keys\n",
    "    keyword_map = {\n",
    "        rf\"(?<![\\w-]){re.escape('non-emergency')}(?![\\w-])\": \"Non-Emergency Support\",\n",
    "        # Matches \"nonemergency\", \"non-emergency\", \"non emergency\"\n",
    "        rf\"(?<![\\w-]){re.escape('emergency')}(?![\\w-])\": \"Emergency Support\",  # Matches standalone \"emergency\"\n",
    "        r\"\\b1st\\s*level\\b|\\b1st\\s*\\+?\\s*level\\s*support\\b\": \"1st Level Support\",\n",
    "        # Matches variations like \"1stlevel\", \"1st level\", \"1st+level support\"\n",
    "        r\"\\b2nd\\s*level\\b|\\b2nd\\s*\\+?\\s*level\\s*support\\b\": \"2nd Level Support\"\n",
    "        # Matches variations like \"2ndlevel\", \"2nd level\", \"2nd+level support\"\n",
    "    }\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row.iloc[0]  # First column (normalized text)\n",
    "        value = row.iloc[1]  # Second column (support hours value)\n",
    "\n",
    "        # print(f\"\\nKey: {text}\")\n",
    "        # print(f\"Value: {value}\\n\")\n",
    "\n",
    "        if pd.notna(value):  # Ensure the value is valid\n",
    "            value = str(value).replace(\"\\n\", \"\").strip()\n",
    "\n",
    "            for regex, category in keyword_map.items():\n",
    "                if re.search(regex, text, re.IGNORECASE):  # Match found (case-insensitive)\n",
    "                    if not support_dict[category]:  # Store only first found value\n",
    "                        support_dict[category] = value\n",
    "\n",
    "    return support_dict\n",
    "\n",
    "\n",
    "# Function to extract all the tables from the support time page\n",
    "def extract_dataframes_from_support_hour_pages(pdf_path, page_numbers):\n",
    "    extracted_dataframes = []\n",
    "\n",
    "    # Define possible valid column headers\n",
    "    valid_headers = [\n",
    "        [\"the service time describes the hours of coverage for this service.\", \"service time\"],\n",
    "        [\"the service time describes the hours of coverage for this service.\",\n",
    "         \"service time (cet if not stated otherwise)\"],\n",
    "        [\"the service time describes the hours of coverage for this service.\",\n",
    "         \"service time (cet/cest if not stated otherwise)\"],\n",
    "        [\"term\", \"service time\"],\n",
    "        [\"term\", \"service time (cet if not stated otherwise)\"],\n",
    "        [\"term\", \"service time (cet/cest if not stated otherwise)\"],\n",
    "        ['term', 'service time (cet if not stated otherwise)']\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        for page_number in page_numbers:\n",
    "            page_number_str = str(page_number)\n",
    "            tables = camelot.read_pdf(pdf_path, pages=page_number_str, flavor='lattice', line_scale=50)\n",
    "\n",
    "            if not tables or tables.n == 0:\n",
    "                continue  # Skip if no tables found\n",
    "\n",
    "            # Convert each table to a DataFrame\n",
    "            for i, table in enumerate(tables):\n",
    "                try:\n",
    "                    df = table.df  # Convert table to DataFrame\n",
    "                    # print(df)\n",
    "\n",
    "                    if df.empty:\n",
    "                        continue  # Skip empty tables\n",
    "\n",
    "                    # Normalize headers by removing extra spaces and converting to lowercase\n",
    "                    headers = [re.sub(r'\\s+', ' ', str(col)).strip().lower() for col in df.iloc[0]]\n",
    "                    headers = [col for col in headers if col]\n",
    "                    # print(f\"\\nExtracted Headers for Table {i+1}: {headers}\")  # Debugging output\n",
    "\n",
    "                    # Check if valid headers exist in extracted headers\n",
    "                    if any(set(valid_set).issubset(set(headers)) for valid_set in valid_headers):\n",
    "                        expected_columns = next(\n",
    "                            valid_set for valid_set in valid_headers if set(valid_set).issubset(set(headers)))\n",
    "                        # Trim df columns to match valid header count\n",
    "                        df = df.iloc[:, :len(expected_columns)]  # Trim extra columns\n",
    "                        df.columns = expected_columns  # Assign the expected headers\n",
    "                        df = df[1:].reset_index(drop=True)  # Drop the first row (headers)\n",
    "                        # print(df)\n",
    "                        extracted_dataframes.append(df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing Table {i + 1}: {e}\")  # Catch block\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing the PDF file: {e}\")\n",
    "\n",
    "    return extracted_dataframes  # Always return a list"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extracting the Run of Service details from the table",
   "id": "86c10b049e99d49f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.701890Z",
     "start_time": "2025-03-28T09:57:17.671512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to extract the ros details from ros tables\n",
    "def extract_ros_details(dataframes):\n",
    "\n",
    "    ros_support_details = {\n",
    "        \"1st Level Support\": \"\",\n",
    "        \"2nd / 3rd Level Support\": \"\"\n",
    "    }\n",
    "\n",
    "    # Variations for 2nd / 3rd Level Support\n",
    "    second_third_variations = [\n",
    "        \"2nd / 3rd Level Support\",\n",
    "        \"2nd/3rd Level Support\",\n",
    "        \"2nd Level Support\",\n",
    "        \"3rd Level Support\"\n",
    "    ]\n",
    "\n",
    "    # Check each dataframe\n",
    "    for df in dataframes:\n",
    "        # Convert to string to safely handle mixed data\n",
    "        df_str = df.astype(str)\n",
    "\n",
    "        # Iterate over rows\n",
    "        for row_index in range(len(df_str)):\n",
    "            row_values = df_str.iloc[row_index].tolist()\n",
    "\n",
    "            # Iterate over each cell in the row to find keywords\n",
    "            for col_idx, cell_value in enumerate(row_values):\n",
    "\n",
    "                # 1) Check for \"1st Level Support\"\n",
    "                if \"1st Level Support\".lower() in cell_value.lower():\n",
    "                    # Gather all columns in this row that contain a tick\n",
    "                    ticked_columns = []\n",
    "                    for check_col_idx, cell_content in enumerate(row_values):\n",
    "                        if check_col_idx != col_idx and (\"✓\" in cell_content or \"\" in cell_content):\n",
    "                            ticked_columns.append(df_str.columns[check_col_idx])\n",
    "\n",
    "                    # Store comma-separated column headers if any\n",
    "                    if ticked_columns:\n",
    "                        ros_support_details[\"1st Level Support\"] = \", \".join(ticked_columns)\n",
    "\n",
    "                # 2) Check for any variant of \"2nd / 3rd Level Support\"\n",
    "                for variant in second_third_variations:\n",
    "                    if variant.lower() in cell_value.lower():\n",
    "                        ticked_columns = []\n",
    "                        for check_col_idx, cell_content in enumerate(row_values):\n",
    "                            if check_col_idx != col_idx and (\"✓\" in cell_content or \"\" in cell_content):\n",
    "                                ticked_columns.append(df_str.columns[check_col_idx])\n",
    "\n",
    "                        if ticked_columns:\n",
    "                            ros_support_details[\"2nd / 3rd Level Support\"] = \", \".join(ticked_columns)\n",
    "\n",
    "    return ros_support_details\n",
    "\n",
    "\n",
    "# Function to check if the extracted headers match the given pattern\n",
    "def header_matches(headers, pattern):\n",
    "\n",
    "    # Wildcard-based check (requires exact column count)\n",
    "    if \"*\" in pattern:\n",
    "        if len(headers) != len(pattern):\n",
    "            return False\n",
    "        for h, p in zip(headers, pattern):\n",
    "            if p != \"*\" and h.lower() != p.lower():\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        # For non-wildcard patterns, we check for subset presence (order is not important)\n",
    "        pattern_lower = set(item.lower() for item in pattern)\n",
    "        headers_lower = set(item.lower() for item in headers)\n",
    "        return pattern_lower.issubset(headers_lower)\n",
    "\n",
    "\n",
    "# Function to extract all the tables from the ros pages\n",
    "def extract_dataframes_from_ros_pages(pdf_path, page_numbers):\n",
    "\n",
    "    extracted_dataframes = []\n",
    "\n",
    "    # Sample list of values to check\n",
    "    target_values = [\n",
    "        \"Delivery Support Process\", \"Delivery Support Processes\",\n",
    "        \"Support Processes\", \"Support Process\",\n",
    "        \"Service Processes\", \"Service Process\", \"Process Category\"\n",
    "    ]\n",
    "\n",
    "    # Define possible valid column headers\n",
    "    valid_headers = [\n",
    "        [\"Delivery Support Process\", \"Yes\", \"No\"],\n",
    "        [\"Delivery Support Processes\", \"Yes\", \"No\"],\n",
    "        [\"Support Processes\", \"Yes\", \"No\"],\n",
    "        [\"Service Processes\", \"Yes\", \"No\"],\n",
    "        [\"Process Category\", \"Delivery Support Process\", \"Yes\", \"No\"],\n",
    "        [\"Process Category\", \"Delivery Support Processes\", \"Yes\", \"No\"],\n",
    "        [\"Process Category\", \"Support Processes\", \"Yes\", \"No\"],\n",
    "        [\"Process Category\", \"Service Processes\", \"Yes\", \"No\"],\n",
    "        [\"Delivery Support Process\", \"*\", \"*\"],\n",
    "        [\"Delivery Support Process\", \"*\", \"*\", \"*\"]\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        for page_number in page_numbers:\n",
    "            page_number_str = str(page_number)\n",
    "            tables = camelot.read_pdf(pdf_path, pages=page_number_str, flavor='lattice', line_scale=50)\n",
    "\n",
    "            if not tables or tables.n == 0:\n",
    "                continue  # Skip if no tables found\n",
    "\n",
    "            # Convert each table to a DataFrame\n",
    "            for i, table in enumerate(tables):\n",
    "                try:\n",
    "                    df = table.df  # Convert table to DataFrame\n",
    "                    df = df.replace(r'^\\s*$', None, regex=True)\n",
    "                    # print(df)\n",
    "\n",
    "                    if df.empty:\n",
    "                        continue  # Skip empty tables\n",
    "\n",
    "                    # Assume first column is the one with categories\n",
    "                    col_name = df.columns[0]\n",
    "\n",
    "                    # Reset index to make slicing easier\n",
    "                    df = df.reset_index(drop=True)\n",
    "\n",
    "                    # Count target values and identify where any value occurs twice\n",
    "                    seen = {}\n",
    "                    cut_index = None\n",
    "\n",
    "                    for idx, val in df[col_name].items():\n",
    "                        if val in target_values:\n",
    "                            seen[val] = seen.get(val, 0) + 1\n",
    "                            if seen[val] == 2:\n",
    "                                cut_index = idx\n",
    "                                break\n",
    "\n",
    "                    # Trim the DataFrame if a duplicate was found\n",
    "                    if cut_index is not None:\n",
    "                        df = df.iloc[:cut_index]\n",
    "\n",
    "                    # Final cleaned DataFrame\n",
    "                    df = df.dropna(axis=1, how='all')\n",
    "                    # print(df)\n",
    "\n",
    "                    # Normalize headers by removing extra spaces and converting to lowercase\n",
    "                    headers = [re.sub(r'\\s+', ' ', str(col)).strip() for col in df.iloc[0]]\n",
    "                    # print(f\"\\nExtracted Headers for Table {i+1}: {headers}\")\n",
    "                    headers = [col for col in headers if col]\n",
    "                    # print(f\"\\nExtracted Headers for Table {i+1}: {headers}\")    # Debug\n",
    "\n",
    "                    # Check if the table's headers match any of our valid header patterns\n",
    "                    if any(header_matches(headers, valid_pattern) for valid_pattern in valid_headers):\n",
    "                        df.columns = headers  # Assign the headers\n",
    "                        df = df[1:].reset_index(drop=True)  # Drop the header row\n",
    "                        extracted_dataframes.append(df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing Table {i + 1}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing the PDF file: {e}\")\n",
    "\n",
    "    return extracted_dataframes  # Always return a list"
   ],
   "id": "81455c9c6f8cc120",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save the extracted data to Excel",
   "id": "b31b1700-15fa-4d16-9ec4-519b8e2e96cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.734716Z",
     "start_time": "2025-03-28T09:57:17.712933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to extract only numeric values from the Availability data\n",
    "def extract_numeric_availability(availability):\n",
    "    if isinstance(availability, str):\n",
    "        # Replace commas with dots for European-style decimals\n",
    "        availability = availability.replace(',', '.')\n",
    "        # Extract only decimal or integer values after '>=', '=', or space, and remove unnecessary text\n",
    "        numbers = re.findall(r'[>=\\s]*?(\\d+\\.\\d+|\\d+)\\s*%', availability)\n",
    "        return \", \".join(numbers) if numbers else \"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Function to store the extracted values to the Excel\n",
    "def insert_data_to_excel(excel_path, bsn_value, response_time_list, resolution_time_list, extracted_material_data,\n",
    "                         extracted_drc_value, support_hour_dict, ros_support_details):\n",
    "    # Required columns\n",
    "    columns = [\n",
    "        \"BSN Number\", \"Material No/Nos\", \"Availability\",\n",
    "        \"Response Time P1\", \"Response Time P2\", \"Response Time P3\", \"Response Time P4\",\n",
    "        \"Resolution Time P1\", \"Resolution Time P2\", \"Resolution Time P3\", \"Resolution Time P4\",\n",
    "        \"DRC Service\", \"Applicable DRC\", \"Applicable RPO\", \"1st Level Support\", \"2nd Level Support\",\n",
    "        \"Emergency Support\", \"Non-Emergency Support\", \"1st Level Support Provided\", \"2nd/3rd Level Support Provided\"\n",
    "    ]\n",
    "\n",
    "    # Load existing Excel file or create a new DataFrame\n",
    "    if os.path.isfile(excel_path):\n",
    "        df = pd.read_excel(excel_path, dtype=str)  # Load as strings to avoid type mismatches\n",
    "        if \"BSN Number\" in df.columns and bsn_value in df[\"BSN Number\"].values:\n",
    "            print(f\"\\nBSN Number {bsn_value} already exists in the Excel file. Skipping insertion.\")\n",
    "            return  # Skip insertion\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=columns)  # Create new dataframe if file does not exist\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    # --- Material No & Availability ---\n",
    "    material_no_list, availability_values = [], []\n",
    "\n",
    "    for material, availability in extracted_material_data.items():\n",
    "        if isinstance(availability, list):\n",
    "            processed_availability = [extract_numeric_availability(val) for val in availability if val.strip()]\n",
    "            processed_availability = [val for val in processed_availability if val]\n",
    "            if not processed_availability:\n",
    "                processed_availability = [\"\"]\n",
    "            for avail in processed_availability:\n",
    "                material_no_list.append(material)\n",
    "                availability_values.append(avail)\n",
    "        else:\n",
    "            material_no_list.append(material)\n",
    "            availability_values.append(extract_numeric_availability(availability))\n",
    "\n",
    "    # --- DRC Values ---\n",
    "    drc_service_list, drc_rto_list, drc_rpo_list = [], [], []\n",
    "\n",
    "    for drc_service, values in extracted_drc_value.items():\n",
    "        applicable_drc = values.get(\"Applicable DRC\", []) or [\"\"]\n",
    "        applicable_rpo = values.get(\"Applicable RPO\", []) or [\"\"]\n",
    "\n",
    "        max_length = max(len(applicable_drc), len(applicable_rpo))\n",
    "        for i in range(max_length):\n",
    "            drc_rto = applicable_drc[i] if i < len(applicable_drc) else \"\"\n",
    "            drc_rpo = applicable_rpo[i] if i < len(applicable_rpo) else \"\"\n",
    "            drc_service_list.append(drc_service)\n",
    "            drc_rto_list.append(drc_rto)\n",
    "            drc_rpo_list.append(drc_rpo)\n",
    "\n",
    "    # --- Determine max rows needed ---\n",
    "    max_rows = max(len(drc_service_list), len(material_no_list), 1)\n",
    "\n",
    "    def expand_list(lst):\n",
    "        return lst + [lst[-1] if lst else \"\"] * (max_rows - len(lst))\n",
    "\n",
    "    material_no_list = expand_list(material_no_list)\n",
    "    availability_values = expand_list(availability_values)\n",
    "    drc_service_list = expand_list(drc_service_list)\n",
    "    drc_rto_list = expand_list(drc_rto_list)\n",
    "    drc_rpo_list = expand_list(drc_rpo_list)\n",
    "\n",
    "    # --- Expand new support detail columns ---\n",
    "    first_level_support_provided = ros_support_details.get(\"1st Level Support\", \"\")\n",
    "    second_third_level_support_provided = ros_support_details.get(\"2nd / 3rd Level Support\", \"\")\n",
    "    first_level_support_list = [first_level_support_provided] * max_rows\n",
    "    second_level_support_list = [second_third_level_support_provided] * max_rows\n",
    "\n",
    "    # --- Row creation ---\n",
    "    for i in range(max_rows):\n",
    "        row = {\n",
    "            \"BSN Number\": bsn_value,\n",
    "            \"Material No/Nos\": material_no_list[i],\n",
    "            \"Availability\": availability_values[i],\n",
    "            \"Response Time P1\": response_time_list[0] if len(response_time_list) > 0 else \"\",\n",
    "            \"Response Time P2\": response_time_list[1] if len(response_time_list) > 1 else \"\",\n",
    "            \"Response Time P3\": response_time_list[2] if len(response_time_list) > 2 else \"\",\n",
    "            \"Response Time P4\": response_time_list[3] if len(response_time_list) > 3 else \"\",\n",
    "            \"Resolution Time P1\": resolution_time_list[0] if len(resolution_time_list) > 0 else \"\",\n",
    "            \"Resolution Time P2\": resolution_time_list[1] if len(resolution_time_list) > 1 else \"\",\n",
    "            \"Resolution Time P3\": resolution_time_list[2] if len(resolution_time_list) > 2 else \"\",\n",
    "            \"Resolution Time P4\": resolution_time_list[3] if len(resolution_time_list) > 3 else \"\",\n",
    "            \"DRC Service\": drc_service_list[i],\n",
    "            \"Applicable DRC\": drc_rto_list[i],\n",
    "            \"Applicable RPO\": drc_rpo_list[i],\n",
    "            \"1st Level Support\": support_hour_dict.get(\"1st Level Support\", \"\"),\n",
    "            \"2nd Level Support\": support_hour_dict.get(\"2nd Level Support\", \"\"),\n",
    "            \"Emergency Support\": support_hour_dict.get(\"Emergency Support\", \"\"),\n",
    "            \"Non-Emergency Support\": support_hour_dict.get(\"Non-Emergency Support\", \"\"),\n",
    "            \"1st Level Support Provided\": first_level_support_list[i],\n",
    "            \"2nd/3rd Level Support Provided\": second_level_support_list[i]\n",
    "        }\n",
    "        new_rows.append(row)\n",
    "\n",
    "    # --- Append and display ---\n",
    "    updated_df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    print(f\"\\n{updated_df}\")\n",
    "    # updated_df.to_excel(excel_path, index=False)\n",
    "\n",
    "    # print(f\"\\nAll data inserted into {excel_path} successfully.\")"
   ],
   "id": "34a58f82-949e-4efc-800b-b949f89df597",
   "outputs": [],
   "execution_count": 154
  },
  {
   "cell_type": "markdown",
   "id": "b90a9ce8-8a8a-4b82-97b5-cfb9f177c797",
   "metadata": {},
   "source": [
    "# Example test case using single pdf"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac3fb722-a9e1-40cc-bb92-6b8037abde4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:17.770199Z",
     "start_time": "2025-03-28T09:57:17.756846Z"
    }
   },
   "source": [
    "def main(pdf_path, excel_path):\n",
    "\n",
    "    ### Main code to extract BSN details\n",
    "    bsn_table_df = extract_bsn_table_from_pdf(pdf_path)\n",
    "    bsn_value = extract_bsn_number_from_table(bsn_table_df)\n",
    "    #----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    # ### Main code to extract Incident details\n",
    "    # print(\"\\nThe Incident details:\")\n",
    "    # print(\"-------------------------\")\n",
    "    #\n",
    "    # response_time_list, resolution_time_list = [\"\"] * 4, [\"\"] * 4\n",
    "    #\n",
    "    # incident_search_text = r\"Table\\s+\\d+: Incident (Response and Resolution Time|Response Time|Resolution Time)\"\n",
    "    # incident_page_number = find_incident_table_page_number(pdf_path, incident_search_text)\n",
    "    #\n",
    "    # if not incident_page_number:\n",
    "    #     print(\"\\nIncident details are not available in the SD file.\")\n",
    "    #     print(\"\\nIncident Response Time:\", response_time_list)\n",
    "    #     print(\"\\nIncident Resolution Time:\", resolution_time_list)\n",
    "    #\n",
    "    # else:\n",
    "    #     extracted_dataframe = extract_all_tables_from_incident_page(pdf_path, incident_page_number)\n",
    "    #     extracted_dataframe = extracted_dataframe.replace(\"\", float(\"nan\")).dropna(how='all').reset_index(drop=True)\n",
    "    #     # print(extracted_dataframe)\n",
    "    #     response_time_list, resolution_time_list = convert_df_into_list(extracted_dataframe)\n",
    "    #\n",
    "    #     print(\"\\nIncident Response Time:\", response_time_list)\n",
    "    #     print(\"\\nIncident Resolution Time:\", resolution_time_list)\n",
    "    #----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    # ### Main code to extract Service Availability details\n",
    "    # print(\"\\nThe Service Availability details:\")\n",
    "    # print(\"-------------------------------------\")\n",
    "    #\n",
    "    # empty_dict = {\"\": \"\"}\n",
    "    #\n",
    "    # material_search_start_text = r\"\\d+\\.\\d+(\\.\\d+)?\\sService Availability\"\n",
    "    # material_search_end_text = r\"\\d+\\.\\d+(\\.\\d+)?\\sService (Performance|Reliability|Times)\"\n",
    "    #\n",
    "    # start_pages, end_pages, material_page_numbers = create_page_list(pdf_path, material_search_start_text,\n",
    "    #                                                                  material_search_end_text)\n",
    "    #\n",
    "    # # print(f\"Found Start Pages: {start_pages}\")\n",
    "    # # print(f\"Found End Pages: {end_pages}\")\n",
    "    # # print(f\"Page Range: {material_page_numbers}\")\n",
    "    #\n",
    "    # if material_page_numbers:\n",
    "    #     extracted_material_data = extract_data_from_material_tables(pdf_path, material_page_numbers)\n",
    "    #     if len(extracted_material_data) == 0:\n",
    "    #         print(\"\\nService Availability details are not available in the SD file.\\n\")\n",
    "    #         print(empty_dict)\n",
    "    #     else:\n",
    "    #         print(f\"\\n{extracted_material_data}\")\n",
    "    # else:\n",
    "    #     print(\"\\nService Availability details are not available in the SD file.\")\n",
    "    #----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    # ### Main code to extract DRC details\n",
    "    # print(\"\\nThe DRC details:\")\n",
    "    # print(\"--------------------\")\n",
    "    #\n",
    "    # drc_search_text = r\"Table.*?Service(?:\\s+\\S+){0,6}\\s+(Disaster|Recovery|Revocery|DR)(?:\\s+\\S+){0,6}\\s+\\b(Class|Classes|classes)\\b\"\n",
    "    # drc_page_number = find_drc_table_page_number(pdf_path, drc_search_text)\n",
    "    #\n",
    "    # if drc_page_number:\n",
    "    #     # print(f\"\\nPage number with the target table: {drc_page_number}\")\n",
    "    #\n",
    "    #     extracted_drc_tables = extract_all_tables_from_drc_page(pdf_path, drc_page_number)\n",
    "    #\n",
    "    #     # for extracted_drc_table in extracted_drc_tables:\n",
    "    #     #     print(f\"\\n{extracted_drc_table}\")\n",
    "    #\n",
    "    #     if extracted_drc_tables is not None:\n",
    "    #         extracted_drc_value = convert_df_to_dict(extracted_drc_tables)\n",
    "    #     else:\n",
    "    #         extracted_drc_value = convert_df_to_dict(extracted_drc_tables)\n",
    "    #\n",
    "    # else:\n",
    "    #     print(f\"\\nDRC details are not available in the SD file.\")\n",
    "    #     extracted_drc_tables = None\n",
    "    #     extracted_drc_value = convert_df_to_dict(extracted_drc_tables)\n",
    "    #\n",
    "    # print(f\"\\n{extracted_drc_value}\")\n",
    "    #----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    # ### Main code to extract Support Hour details\n",
    "    # print(\"\\nThe Support Hour details:\")\n",
    "    # print(\"----------------------------\")\n",
    "    #\n",
    "    # support_hour_start_text = r\"\\d+\\.\\d+(\\.\\d+)?\\sService (Time|Times)\"\n",
    "    # support_hour_end_text = r\"\\d+\\.\\d+(\\.\\d+)?\\sIncident (Management|Response Time|Resolution Time)\"\n",
    "    #\n",
    "    # start_pages, end_pages, support_hour_pages = create_page_list(pdf_path, support_hour_start_text,\n",
    "    #                                                               support_hour_end_text)\n",
    "    #\n",
    "    # # print(f\"Found Start Pages: {start_pages}\")\n",
    "    # # print(f\"Found End Pages: {end_pages}\")\n",
    "    # # print(f\"Page Range: {support_hour_pages}\")\n",
    "    #\n",
    "    # if support_hour_pages:\n",
    "    #     dataframes = extract_dataframes_from_support_hour_pages(pdf_path, support_hour_pages)\n",
    "    #\n",
    "    #     # Initialize support hour dictionary\n",
    "    #     support_hour_dict = {\n",
    "    #         \"1st Level Support\": \"\",\n",
    "    #         \"2nd Level Support\": \"\",\n",
    "    #         \"Emergency Support\": \"\",\n",
    "    #         \"Non-Emergency Support\": \"\"\n",
    "    #     }\n",
    "    #\n",
    "    #     for dataframe in dataframes:\n",
    "    #         temp_support_hour_values = extract_support_hours(dataframe)\n",
    "    #\n",
    "    #         # Merge extracted values into support_hour_dict\n",
    "    #         for key in support_hour_dict.keys():\n",
    "    #             if temp_support_hour_values[key]:  # Only update if a value exists\n",
    "    #                 support_hour_dict[key] = temp_support_hour_values[key]\n",
    "    #\n",
    "    # else:\n",
    "    #     support_hour_dict = {\n",
    "    #         \"1st Level Support\": \"\",\n",
    "    #         \"2nd Level Support\": \"\",\n",
    "    #         \"Emergency Support\": \"\",\n",
    "    #         \"Non-Emergency Support\": \"\"\n",
    "    #     }\n",
    "    #\n",
    "    # # print(f\"\\n{support_hour_dict}\\n\")\n",
    "    #\n",
    "    # # Print formatted output\n",
    "    # for key, value in support_hour_dict.items():\n",
    "    #     print(f\"{key}: {value if value else ''}\")\n",
    "    #----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    ### Main code to extract Run of Service\n",
    "    print(\"\\nThe Run of Service details:\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    ros_support_details = {\n",
    "        \"1st Level Support\": \"\",\n",
    "        \"2nd / 3rd Level Support\": \"\"\n",
    "    }\n",
    "\n",
    "    ros_start_text = r\"\\d+\\.\\d+(?:\\.\\d+)?\\s(?:\\w+\\s)?Run of Service\"\n",
    "    ros_end_text = r\"\\d+\\.\\d+(?:\\.\\d+)?\\sRetirement of(?:\\s\\w+)? Service\"\n",
    "\n",
    "\n",
    "    start_pages, end_pages, ros_page_numbers = create_page_list(pdf_path, ros_start_text, ros_end_text)\n",
    "\n",
    "    # print(f\"Found Start Pages: {start_pages}\")\n",
    "    # print(f\"Found End Pages: {end_pages}\")\n",
    "    # print(f\"Page Range: {ros_page_numbers}\")\n",
    "\n",
    "    # If we don't have valid pages, return an empty dict\n",
    "    if not ros_page_numbers:\n",
    "        print(\"\\nNo 'Run of Service' section found in the document.\")\n",
    "        return ros_support_details\n",
    "\n",
    "    # Extract tables from the identified pages\n",
    "    extracted_ros_tables = extract_dataframes_from_ros_pages(pdf_path, ros_page_numbers)\n",
    "\n",
    "    # Print each extracted table (for debugging/verification)\n",
    "    for idx, table in enumerate(extracted_ros_tables, start=1):\n",
    "        print(f\"\\n--- ROS Table {idx} ---\\n{table}\")\n",
    "\n",
    "    # Extract ROS support details from the tables\n",
    "    ros_support_details = extract_ros_details(extracted_ros_tables)\n",
    "    print(ros_support_details)\n",
    "    #----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    # ### Main code to insert all the extracted data into the Excel\n",
    "    # print(\"\\n\\nGetting data into the Excel...\\n\")\n",
    "    #\n",
    "    # insert_data_to_excel(excel_path, bsn_value, response_time_list, resolution_time_list, extracted_material_data,\n",
    "    #                      extracted_drc_value, support_hour_dict, ros_support_details)"
   ],
   "outputs": [],
   "execution_count": 155
  },
  {
   "cell_type": "code",
   "id": "fc251d4d-afae-4162-bf5a-bf8c1fd21391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:29.577155Z",
     "start_time": "2025-03-28T09:57:17.773412Z"
    }
   },
   "source": [
    "# Define paths\n",
    "folder_path = r\"C:/Users/rmya5fe/OneDrive - Allianz/01_Automated Reports/07_Sample_SDs\"\n",
    "database_path = os.path.join(folder_path, \"Database\")\n",
    "excel_path = os.path.join(folder_path, \"02_SLA_extract_from_SD.xlsx\")\n",
    "# excel_path = os.path.join(folder_path, \"Database\\_sample_sd\\SLA_extract_from_SD.xlsx\")\n",
    "\n",
    "# Example usage\n",
    "file_name = \"8000047_JBoss_SD_v02_01.pdf\"  # Example file name\n",
    "pdf_path = os.path.join(database_path, file_name)\n",
    "main(pdf_path, excel_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted BSN Number: BSN8000047\n",
      "\n",
      "The Run of Service details:\n",
      "-------------------------\n",
      "\n",
      "--- ROS Table 1 ---\n",
      "                    Delivery Support Process   Yes    No\n",
      "0                          1st Level Support       None\n",
      "1                    2nd / 3rd Level Support       None\n",
      "2                          Access Management       None\n",
      "3                     Application Management  None     \n",
      "4         Availability & Capacity Management       None\n",
      "5                          Change Management       None\n",
      "6                        Incident Management       None\n",
      "7            Information Security Management       None\n",
      "8     Infrastructure & Application Operation       None\n",
      "9                       Knowledge Management       None\n",
      "10                        Problem Management       None\n",
      "11           Release & Deployment Management       None\n",
      "12             Request Fulfilment Management       None\n",
      "13  Service Asset & Configuration Management       None\n",
      "{'1st Level Support': 'Yes', '2nd / 3rd Level Support': 'Yes'}\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T09:57:29.607919Z",
     "start_time": "2025-03-28T09:57:29.603839Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ce096bd174bd368e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
